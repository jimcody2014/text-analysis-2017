{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Topic Modeling\n",
    "  \n",
    "Today we'll implement the most basic, and the original, topic modeling algorithm, LDA, using Python's scikit-learn. The other major topic modeling package is Gensim. \n",
    "\n",
    "Another option for topic modeling is the software MALLET. Check out this [blog post](https://de.dariah.eu/tatom/topic_model_mallet.html) to learn more about implementing MALLET.\n",
    "\n",
    "### Learning Goals\n",
    "* Implement a basic topic modeling algorithm and learn how to tweak it\n",
    "* Learn how to use different methods to calculate topic prevalence\n",
    "* Learn how to create some simple graphs with this output\n",
    "* Think though how and why you might use topic modeling in a text analysis project\n",
    "\n",
    "### Outline\n",
    "\n",
    "* [The Pandas Dataframe: Music Reviews](#df)\n",
    "* [Fit an LDA Topic Model using scikit-learn](#fit)\n",
    "* [Document by Topic Distribution](#dtd)\n",
    "* [Words Aligned with each Topic](#words)\n",
    "* [Topic Prevalence](#prev)\n",
    "* [Topics Over Time](#time)\n",
    "\n",
    "\n",
    "### Key Terms\n",
    "* *Topic Modeling*:\n",
    "    * A statistical model to uncover abstract topics within a text. It uses the co-occurrence fo words within documents, compared to their distribution across documents, to uncover these abstract themes. The output is a list of weighted words, which indicate the subject of each topic, and a weight distribution across topics for each document.\n",
    "    \n",
    "* *LDA*:\n",
    "    * Latent Dirichlet Allocation. A implementation of topic modeling that assumes a Dirichlet prior. It does not take document order into account, unlike other topic modeling algorithms.\n",
    "    \n",
    "### Further Resources\n",
    "\n",
    "[More detailed description of implementing LDA using scikit-learn](http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-topics-extraction-with-nmf-lda-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df'></a>\n",
    "### 0. The Pandas Dataframe: Music Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read our music reviews corpus, which is stored as a .csv file on our hard drive, into a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df_lit = pandas.read_csv(\"../Data/childrens_lit.csv.bz2\", sep='\\t', index_col=0, encoding = 'utf-8', compression='bz2')\n",
    "\n",
    "#drop rows where the text is missing.\n",
    "df_lit = df_lit.dropna(subset=['text'])\n",
    "\n",
    "#view the dataframe\n",
    "df_lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit'></a>\n",
    "### 1. Fit a Topic Model, using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to fit the model. This requires the use of CountVecorizer, which we've already used, and the scikit-learn function LatentDirichletAllocation.\n",
    "\n",
    "See [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) for more information about this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####Adopted From: \n",
    "#Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_samples = 2000\n",
    "n_topics = 4\n",
    "n_top_words = 50\n",
    "\n",
    "##This is a function to print out the top words for each topic in a pretty way.\n",
    "#Don't worry too much about understanding every line of this code.\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize our text using CountVectorizer\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.80, min_df=50,\n",
    "                                max_features=None,\n",
    "                                stop_words='english'\n",
    "                                )\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(df_lit.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_topics=%d...\"\n",
    "      % (n_samples, n_topics))\n",
    "\n",
    "#define the lda function, with desired options\n",
    "#Check the documentation, linked above, to look through the options\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=20,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=80.,\n",
    "                                total_samples=n_samples,\n",
    "                                random_state=0)\n",
    "#fit the model\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print the top words per topic, using the function defined above.\n",
    "#Unlike R, which has a built-in function to print top words, we have to write our own for scikit-learn\n",
    "#I think this demonstrates the different aims of the two packages: R is for social scientists, Python for computer scientists\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####Exercise:\n",
    "###Copy and paste the above code and fit a new model, lda_new, by changing some of the parameters. How does this change the output.\n",
    "###Suggestions:\n",
    "## 1. Change the number of topics. \n",
    "## 2. Do not remove stop words. \n",
    "## 3. Change other options, either in the vectorize stage or the LDA model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dtd'></a>\n",
    "### 2. Document by Topic Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we may want to do with the output is find the most representative texts for each topic. A simple way to do this (but not memory efficient), is to merge the topic distribution back into the Pandas dataframe.\n",
    "\n",
    "First get the topic distribution array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_dist = lda.transform(tf)\n",
    "topic_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge back in with the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_dist_df = pandas.DataFrame(topic_dist)\n",
    "df_w_topics = topic_dist_df.join(df_lit)\n",
    "df_w_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can sort the dataframe for the topic of interest, and view the top documents for the topics.\n",
    "Below we sort the documents first by Topic 0 (looking at the top words for this topic I think it's about family, health, and domestic activities), and next by Topic 1 (again looking at the top words I think this topic is about children playing outside in nature). These topics may be a family/nature split?\n",
    "\n",
    "Look at the titles for the two different topics. Look at the gender of the author. Hypotheses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_w_topics[['title', 'author gender', 0]].sort_values(by=[0], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_w_topics[['title', 'author gender', 1]].sort_values(by=[1], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EX: What is the average topic weight by author gender, for each topic?\n",
    "### Grapth these results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='words'></a>\n",
    "### 3. Words Aligned with each Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following DiMaggio et al., we can calculate the total number of words aligned with each topic, and compare by author gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first create word count column\n",
    "\n",
    "df_w_topics['word_count'] = df_w_topics['text'].apply(lambda x: len(str(x).split()))\n",
    "df_w_topics['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#multiple topic weight by word count\n",
    "\n",
    "df_w_topics['0_wc'] = df_w_topics[0] * df_w_topics['word_count']\n",
    "df_w_topics['0_wc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a for loop to do this for every topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_columns = range(0, n_topics)\n",
    "col_list = []\n",
    "for num in topic_columns:\n",
    "    col = \"%d_wc\" % num\n",
    "    col_list.append(col)\n",
    "    #Solution\n",
    "    df_w_topics[col] = df_w_topics[num] * df_w_topics['word_count']\n",
    "    \n",
    "df_w_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EX: What is the total number of words aligned with each topic, by author gender?\n",
    "#EX: What is the proportion of total words aligned with each topic, by author gender?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Why might we want to do one calculation over the other? Take average topic weight per documents versus the average number of words aligned with each topic?\n",
    "\n",
    "This brings us to..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prev'></a>\n",
    "### 4. Topic Prevalence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###EX: \n",
    "#       Find the most prevalent topic in the corpus.\n",
    "#       Find the least prevalent topic in the corpus. \n",
    "#       Hint: How do we define prevalence? What are different ways of measuring this,\n",
    "#              and the benefits/drawbacks of each?       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='time'></a>\n",
    "### 4. Prevalence over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same as above, but by year, to graph the prevalence of each topic over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_year = df_w_topics.groupby('year')\n",
    "fig3 = plt.figure()\n",
    "chrt = 0\n",
    "for e in col_list:\n",
    "    chrt += 1 \n",
    "    ax2 = fig3.add_subplot(2,3, chrt)\n",
    "    (grouped_year[e].sum()/grouped_year['word_count'].sum()).plot(kind='line', title=e)\n",
    "    \n",
    "fig3.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 2 I interpret to be about battles in France. What is going on between 1800 and 1804 in France that might make this topic increasingly popular over this time period?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
